{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polynomial Regression (degree=2):\n",
      "Train MSE: 4.7083, Test MSE: 3.7380\n",
      "Polynomial Regression (degree=3):\n",
      "Train MSE: 4.6784, Test MSE: 3.7068\n",
      "RBF Kernel Regression:\n",
      "Train MSE: 586987.3921, Test MSE: 5007640.9091\n",
      "Linear Regression:\n",
      "Train MSE: 5.1059, Test MSE: 4.1286\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "columns = ['Gender', 'Length', 'Diameter', 'Height', 'Whole_weight', 'Shucked_weight', 'Viscera_weight', 'Shell_weight', 'Rings']\n",
    "data = pd.read_csv('abalone.csv', names=columns)\n",
    "\n",
    "# Drop the gender feature\n",
    "data = data.drop(columns=['Gender'])\n",
    "\n",
    "# Convert the dataframe into a NumPy array for easier processing\n",
    "X = data.iloc[:, :-1].values  # Features (all columns except the last)\n",
    "y = data.iloc[:, -1].values    # Target variable (the last column)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "np.random.seed(42)  # For reproducibility\n",
    "indices = np.random.permutation(len(X))\n",
    "train_size = int(len(X) * 0.8)\n",
    "train_indices, test_indices = indices[:train_size], indices[train_size:]\n",
    "\n",
    "X_train, X_test = X[train_indices], X[test_indices]\n",
    "y_train, y_test = y[train_indices], y[test_indices]\n",
    "\n",
    "# Function to calculate Mean Squared Error\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "# Linear Regression Implementation\n",
    "def linear_regression(X, y):\n",
    "    # Add a bias term (column of ones) to the feature matrix\n",
    "    X_b = np.c_[np.ones((X.shape[0], 1)), X]  # Add x0 = 1 to each instance\n",
    "    theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "    return theta_best\n",
    "\n",
    "# Predict using Linear Regression\n",
    "theta_linear = linear_regression(X_train, y_train)\n",
    "X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]  # Add bias term for test\n",
    "y_pred_train_linear = X_train.dot(theta_linear[1:]) + theta_linear[0]\n",
    "y_pred_test_linear = X_test_b.dot(theta_linear)\n",
    "\n",
    "# Calculate MSE for Linear Regression\n",
    "mse_train_linear = mean_squared_error(y_train, y_pred_train_linear)\n",
    "mse_test_linear = mean_squared_error(y_test, y_pred_test_linear)\n",
    "\n",
    "# Polynomial Regression Implementation\n",
    "def polynomial_features(X, degree):\n",
    "    \"\"\"Generate polynomial features.\"\"\"\n",
    "    return np.column_stack([X ** d for d in range(1, degree + 1)])\n",
    "\n",
    "def polynomial_regression(X, y, degree):\n",
    "    X_poly = polynomial_features(X, degree)\n",
    "    X_poly_b = np.c_[np.ones((X_poly.shape[0], 1)), X_poly]  # Add bias term\n",
    "    theta_best = np.linalg.inv(X_poly_b.T.dot(X_poly_b)).dot(X_poly_b.T).dot(y)\n",
    "    return theta_best\n",
    "\n",
    "# Predict using Polynomial Regression\n",
    "for degree in [2, 3]:\n",
    "    theta_poly = polynomial_regression(X_train, y_train, degree)\n",
    "    X_poly_test = polynomial_features(X_test, degree)\n",
    "    X_poly_train = polynomial_features(X_train, degree)\n",
    "    \n",
    "    y_pred_train_poly = X_poly_train.dot(theta_poly[1:]) + theta_poly[0]\n",
    "    y_pred_test_poly = np.c_[np.ones((X_poly_test.shape[0], 1)), X_poly_test].dot(theta_poly)\n",
    "    \n",
    "    mse_train_poly = mean_squared_error(y_train, y_pred_train_poly)\n",
    "    mse_test_poly = mean_squared_error(y_test, y_pred_test_poly)\n",
    "    \n",
    "    print(f'Polynomial Regression (degree={degree}):')\n",
    "    print(f'Train MSE: {mse_train_poly:.4f}, Test MSE: {mse_test_poly:.4f}')\n",
    "\n",
    "# RBF Kernel Regression Implementation\n",
    "def rbf_kernel(x1, x2, gamma=0.1):\n",
    "    \"\"\"Compute the RBF kernel between two sets of samples.\"\"\"\n",
    "    sq_dists = np.sum(x1**2, axis=1).reshape(-1, 1) + np.sum(x2**2, axis=1) - 2 * np.dot(x1, x2.T)\n",
    "    return np.exp(-gamma * sq_dists)\n",
    "\n",
    "def rbf_regression(X_train, y_train, X_test, gamma=0.1):\n",
    "    K_train = rbf_kernel(X_train, X_train, gamma)\n",
    "    K_test = rbf_kernel(X_test, X_train, gamma)\n",
    "    alpha = np.linalg.inv(K_train).dot(y_train)  # (K_train)^-1 * y_train\n",
    "    y_pred_train = K_train.dot(alpha)\n",
    "    y_pred_test = K_test.dot(alpha)\n",
    "    return y_pred_train, y_pred_test\n",
    "\n",
    "# Predict using RBF Kernel Regression\n",
    "y_pred_train_rbf, y_pred_test_rbf = rbf_regression(X_train, y_train, X_test)\n",
    "\n",
    "# Calculate MSE for RBF Kernel Regression\n",
    "mse_train_rbf = mean_squared_error(y_train, y_pred_train_rbf)\n",
    "mse_test_rbf = mean_squared_error(y_test, y_pred_test_rbf)\n",
    "\n",
    "print(f'RBF Kernel Regression:')\n",
    "print(f'Train MSE: {mse_train_rbf:.4f}, Test MSE: {mse_test_rbf:.4f}')\n",
    "\n",
    "# Final results\n",
    "print(f'Linear Regression:')\n",
    "print(f'Train MSE: {mse_train_linear:.4f}, Test MSE: {mse_test_linear:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
